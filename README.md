# TP Machine Learning - Apprentissage SupervisÃ© & ML Responsable

**Auteur :** Wilfried LEROULIER [Architecte SMA (solutions multi agentique) Capgemini] 
**Date :** 11 DÃ©cembre 2025  
**Contexte :** Travaux Pratiques de Machine Learning (Master/Ã‰cole d'IngÃ©nieurs)

---

## ğŸ“‹ Vue d'ensemble

Ce projet regroupe deux parties complÃ©mentaires d'un TP de Machine Learning :
- **Partie 1 :** Apprentissage supervisÃ© avec arbres de dÃ©cision (dÃ©tection d'intrusion rÃ©seau)
- **Partie 2 :** Machine Learning Responsable & Audit de Fairness (scoring de crÃ©dit)

---

## ğŸ—‚ï¸ Structure du Projet

```
TP_ML_yann_11122025/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Portmap.csv                    # Dataset trafic rÃ©seau (Partie 1)
â”‚   â””â”€â”€ synthetic_credit_scoring.csv   # Dataset crÃ©dit synthÃ©tique (Partie 2)
â”‚
â”œâ”€â”€ TP_Supervise_Arbre.ipynb           # Notebook Partie 1 - Arbres de dÃ©cision
â”œâ”€â”€ TP_Partie2_Fairness.ipynb          # Notebook Partie 2 - Fairness & SHAP
â”œâ”€â”€ generate_dataset.py                # Script de gÃ©nÃ©ration de donnÃ©es (Partie 2)
â”œâ”€â”€ requirements.txt                   # DÃ©pendances Python
â””â”€â”€ README.md                          # Ce fichier
```

---

## ğŸ¯ Partie 1 : Apprentissage SupervisÃ© - DÃ©tection d'Intrusion

### Objectif
Utiliser un **arbre de dÃ©cision** pour classifier du trafic rÃ©seau et dÃ©tecter des attaques de type Portmap.

### Dataset
- **Source :** `data/Portmap.csv` (trafic rÃ©seau capturÃ©)
- **Features :** Flow Duration, Packets, Bytes, Ports, Flags, etc.
- **Target :** Label (BENIGN vs Attaque)

### DÃ©marche

1. **Chargement & Nettoyage**
   - Analyse des types de donnÃ©es
   - Suppression des colonnes non-numÃ©riques (IPs, Timestamps)
   - Encodage du label avec `LabelEncoder`
   - Gestion des valeurs manquantes et infinies
   - Standardisation avec `StandardScaler`

2. **ModÃ©lisation**
   - Division Train/Test (80/20)
   - EntraÃ®nement d'un `DecisionTreeClassifier`
   - Limitation de la profondeur (`max_depth=4`) pour la lisibilitÃ©

3. **Ã‰valuation**
   - Matrice de confusion
   - Classification report (PrÃ©cision, Recall, F1-Score)
   - **Visualisation de l'arbre** : graphique permettant de comprendre les rÃ¨gles de dÃ©cision
   - Importance des features

### Concepts ClÃ©s
- âœ… **PrÃ©paration des donnÃ©es** : Nettoyage, encodage, standardisation obligatoires
- âœ… **Arbre de dÃ©cision** : ModÃ¨le interprÃ©table, visualisable, sensible Ã  l'overfitting
- âœ… **Avantages vs KNN** : Pas besoin de standardisation, rapide en prÃ©diction, facile Ã  expliquer
- âœ… **Ensemble Learning** : AgrÃ©gation de modÃ¨les (Random Forest) pour rÃ©duire la variance

### RÃ©sultats
Le modÃ¨le obtient une accuracy Ã©levÃ©e et identifie correctement les attaques Portmap grÃ¢ce Ã  des rÃ¨gles simples sur la durÃ©e des flux et le nombre de paquets.

---

## âš–ï¸ Partie 2 : Machine Learning Responsable & Fairness

### Objectif
Auditer un modÃ¨le de Machine Learning pour **dÃ©tecter et mesurer des biais discriminatoires** dans un contexte de scoring de crÃ©dit.

### ProblÃ¨me Initial : Absence de Dataset

**DÃ©fi :** Le dataset Ã©tait non disponible.

**Solution AdoptÃ©e : GÃ©nÃ©ration de DonnÃ©es SynthÃ©tiques**

Nous avons crÃ©Ã© un script Python (`generate_dataset.py`) qui :
- GÃ©nÃ¨re 2000 lignes de donnÃ©es de crÃ©dit rÃ©alistes avec `Faker`
- Introduit **volontairement un biais mathÃ©matique** contre les femmes (-15 points de score)
- Simule une discrimination systÃ©mique observable dans les donnÃ©es historiques
- Permet la **reproductibilitÃ©** et l'**auditabilitÃ©** du TP

**Pourquoi cette approche ?**
- âœ… ContrÃ´le total sur le type et l'ampleur du biais
- âœ… Transparence sur l'origine des donnÃ©es
- âœ… Respect de la vie privÃ©e (pas de vraies donnÃ©es personnelles)
- âœ… PÃ©dagogie : on sait exactement ce que le modÃ¨le devrait dÃ©tecter

### Dataset SynthÃ©tique
- **Features :** Age, Gender, Income, Debt_Ratio, Employment_Years, Loan_Amount
- **Target :** Loan_Approved (0 = RefusÃ©, 1 = ApprouvÃ©)
- **Biais introduit :** Les femmes ont ~12-15% de chances en moins d'obtenir un prÃªt Ã  mÃ©rite Ã©gal

### DÃ©marche

1. **GÃ©nÃ©ration & Transparence**
   - ExÃ©cution de `generate_dataset.py`
   - Documentation de la mÃ©thode de gÃ©nÃ©ration dans le notebook
   - Visualisation du biais brut dans les donnÃ©es (graphique par genre)

2. **ModÃ©lisation**
   - EntraÃ®nement d'un `RandomForestClassifier`
   - Le modÃ¨le apprend sur des donnÃ©es biaisÃ©es (situation rÃ©aliste)

3. **Audit de Fairness**
   - **Recall par groupe** : Mesure de la sensibilitÃ© pour Hommes vs Femmes
   - **Disparate Impact** : Ratio d'approbation (Femmes / Hommes)
   - **RÃ¨gle des 80%** : Si DI < 0.8 â†’ Discrimination significative

4. **ExplicabilitÃ© (SHAP)**
   - Calcul des valeurs SHAP avec `TreeExplainer`
   - Summary plot : Identification des features les plus influentes
   - VÃ©rification si le genre est directement utilisÃ© pour discriminer

### Concepts ClÃ©s

#### ğŸ” Audit de Fairness
- **Disparate Impact** : MÃ©trique lÃ©gale (norme USA) mesurant les discriminations
- **Group Fairness** : Comparaison des performances entre groupes protÃ©gÃ©s
- **Recall par groupe** : Ã‰vite qu'une bonne accuracy globale cache des disparitÃ©s

#### ğŸ§  ExplicabilitÃ© (SHAP)
- **SHAP Values** : Mesure de la contribution de chaque feature Ã  chaque prÃ©diction
- **Discrimination directe** : Le modÃ¨le utilise explicitement le genre
- **Proxy discrimination** : Le modÃ¨le utilise des variables corrÃ©lÃ©es au genre (salaire, emploi)

#### âš ï¸ LeÃ§ons Ã‰thiques
1. **Un algorithme n'est jamais neutre** : Il reflÃ¨te les biais des donnÃ©es
2. **L'accuracy ne suffit pas** : Un modÃ¨le prÃ©cis peut Ãªtre injuste
3. **L'audit est obligatoire** : Sans mesures de fairness, les biais passent inaperÃ§us
4. **La transparence protÃ¨ge** : Documenter les choix et limitations est essentiel

### RÃ©sultats
- Le modÃ¨le atteint une bonne prÃ©cision globale (~85%)
- **Mais** le Disparate Impact est < 0.8 â†’ Discrimination dÃ©tectÃ©e
- SHAP rÃ©vÃ¨le que le genre est une variable influente dans les dÃ©cisions
- **Conclusion** : Le modÃ¨le a appris et reproduit le biais discriminatoire

---

## ğŸ› ï¸ Installation & Utilisation

### PrÃ©requis
- Python 3.8+
- VS Code (recommandÃ©)
- Terminal Bash (Git Bash sur Windows)

### Installation

```bash
# 1. CrÃ©er l'environnement virtuel
python -m venv venv

# 2. Activer l'environnement (Windows Bash)
source venv/Scripts/activate

# 3. Installer les dÃ©pendances
pip install -r requirements.txt
```

### ExÃ©cution

#### Partie 1 - Arbres de DÃ©cision
```bash
# Ouvrir le notebook
jupyter notebook TP_Supervise_Arbre.ipynb

# ExÃ©cuter les cellules sÃ©quentiellement
```

#### Partie 2 - Fairness
```bash
# 1. GÃ©nÃ©rer le dataset synthÃ©tique
python generate_dataset.py

# 2. Ouvrir le notebook
jupyter notebook TP_Partie2_Fairness.ipynb

# 3. ExÃ©cuter les cellules sÃ©quentiellement
```

---

## ğŸ“¦ DÃ©pendances Principales

| Package | Usage |
|---------|-------|
| `pandas` | Manipulation de donnÃ©es |
| `numpy` | Calculs numÃ©riques |
| `scikit-learn` | ModÃ¨les ML, mÃ©triques |
| `matplotlib` | Visualisations de base |
| `seaborn` | Visualisations avancÃ©es |
| `shap` | ExplicabilitÃ© des modÃ¨les |
| `fairlearn` | Audit de fairness (Partie 2) |
| `faker` | GÃ©nÃ©ration de fausses donnÃ©es |
| `jupyter` | Notebooks interactifs |

---

## ğŸ“ Concepts Fondamentaux Ã  Retenir

### Machine Learning SupervisÃ©

1. **PrÃ©paration des donnÃ©es = 80% du travail**
   - Nettoyage, encodage, gestion des manquants
   - Standardisation pour la plupart des algos (pas pour les arbres)
   - Pas de donnÃ©es â†’ Pas de modÃ¨le fonctionnel

2. **Trade-off InterprÃ©tabilitÃ© vs Performance**
   - Arbres : Faciles Ã  expliquer, visualisables
   - Random Forest : Plus performantes, moins lisibles
   - RÃ©seaux de neurones : TrÃ¨s performants, boÃ®tes noires

3. **Validation rigoureuse**
   - Train/Test split obligatoire
   - Matrice de confusion pour comprendre les erreurs
   - MÃ©triques multiples (Accuracy, Precision, Recall, F1)

### Machine Learning Responsable

4. **Biais algorithmiques**
   - Les algorithmes reflÃ¨tent les biais des donnÃ©es historiques
   - L'Ã©quitÃ© ne vient pas "par dÃ©faut"
   - L'audit est une obligation Ã©thique et lÃ©gale

5. **MÃ©triques de Fairness**
   - **Disparate Impact** : Ratio de succÃ¨s entre groupes
   - **Equal Opportunity** : Recall Ã©quilibrÃ© entre groupes
   - **Calibration** : Confiance Ã©quitable dans les prÃ©dictions

6. **ExplicabilitÃ©**
   - SHAP : Valeurs de Shapley pour l'attribution de prÃ©diction
   - LIME : Explication locale par approximation linÃ©aire
   - Importance des features : Quelles variables comptent le plus ?

7. **Gouvernance & Documentation**
   - Documenter les choix de design
   - Tracer l'origine des donnÃ©es
   - Rendre le systÃ¨me auditable
   - PrÃ©voir une supervision humaine pour les dÃ©cisions critiques

---

## ğŸ”„ DÃ©marche Scientifique AppliquÃ©e

### ProblÃ¨me â†’ Solution

| ProblÃ¨me RencontrÃ© | Solution AdoptÃ©e |
|-------------------|-----------------|
| Dataset Portmap incomplet (colonnes manquantes) | DÃ©tection automatique des colonnes disponibles |
| Colonnes non-numÃ©riques (IPs) | Suppression automatique via `dtype == 'object'` |
| Valeurs infinies dans le dataset | Remplacement par NaN puis imputation par la moyenne |
| Dataset de fairness inexistant | GÃ©nÃ©ration synthÃ©tique contrÃ´lÃ©e avec biais intentionnel |
| Version de SHAP incertaine | DÃ©tection automatique de la structure (list vs array) |

### Bonnes Pratiques AppliquÃ©es
- âœ… Code commentÃ© et cellules Markdown explicatives
- âœ… Gestion d'erreurs robuste (try/except implicites)
- âœ… Visualisations claires avec lÃ©gendes et titres
- âœ… Transparence sur les limitations et choix mÃ©thodologiques
- âœ… ReproductibilitÃ© (seeds, scripts de gÃ©nÃ©ration)

---

## ğŸ“š RÃ©fÃ©rences & Ressources

### Documentation Technique
- [Scikit-Learn Documentation](https://scikit-learn.org/stable/)
- [SHAP Documentation](https://shap.readthedocs.io/)
- [Fairlearn Guide](https://fairlearn.org/)

### Articles AcadÃ©miques
- "Fairness and Machine Learning" - Barocas, Hardt, Narayanan (2019)
- "A Survey on Bias and Fairness in Machine Learning" - Mehrabi et al. (2021)

### Outils de Gouvernance
- [AI Ethics Guidelines](https://ec.europa.eu/digital-strategy/en/policies/expert-group-ai)
- [Model Cards for Model Reporting](https://arxiv.org/abs/1810.03993)

---

## ğŸ¯ CompÃ©tences DÃ©veloppÃ©es

### Techniques
- âœ… Manipulation de donnÃ©es avec Pandas
- âœ… ModÃ©lisation ML avec Scikit-Learn
- âœ… Visualisation avec Matplotlib/Seaborn
- âœ… ExplicabilitÃ© avec SHAP
- âœ… Audit de fairness algorithmique
- âœ… GÃ©nÃ©ration de donnÃ©es synthÃ©tiques

### MÃ©thodologiques
- âœ… DÃ©marche scientifique (ProblÃ¨me â†’ HypothÃ¨se â†’ ExpÃ©rimentation â†’ Conclusion)
- âœ… Documentation technique
- âœ… Gestion de versions (Git)
- âœ… ReproductibilitÃ© des expÃ©riences
- âœ… PensÃ©e critique sur les biais

### Ã‰thiques & Professionnelles
- âœ… Conscience des enjeux de discrimination algorithmique
- âœ… Transparence sur les limitations des modÃ¨les
- âœ… Respect de la vie privÃ©e (donnÃ©es synthÃ©tiques)
- âœ… ResponsabilitÃ© dans le dÃ©ploiement de systÃ¨mes ML

---

## âš ï¸ Limitations & Perspectives

### Limitations Actuelles
- Dataset synthÃ©tique simplifiÃ© (biais unidimensionnel)
- Pas de correction du biais appliquÃ©e (seulement dÃ©tection)
- Pas de test sur donnÃ©es rÃ©elles (RGPD)
- ModÃ¨les simples (pas de deep learning)

### Pistes d'AmÃ©lioration
- ImplÃ©menter des techniques de debiasing (Fairlearn, reweighting)
- Tester d'autres modÃ¨les (XGBoost, rÃ©seaux de neurones)
- Ajouter une interface de dÃ©monstration (Streamlit)
- Comparer plusieurs mÃ©triques de fairness (Equalized Odds, Calibration)
- ImplÃ©menter un monitoring continu du biais en production

---

## ğŸ‘¤ Auteur & Contact

**Wilfried LEROULIER**  
Ã‰tudiant en IA  
ğŸ“§ Contact : mistwil777 
ğŸ“§ Mail : wilfried.leroulier@ecoles-epsi.net

**Note :** Ce projet est rÃ©alisÃ© dans un cadre pÃ©dagogique. Les donnÃ©es sont synthÃ©tiques et ne reflÃ¨tent aucune situation rÃ©elle. L'objectif est d'apprendre les techniques d'audit de fairness dans un environnement contrÃ´lÃ©.

---

## ğŸ“„ Licence

Ce projet est rÃ©alisÃ© dans un cadre acadÃ©mique. Le code est mis Ã  disposition Ã  des fins Ã©ducatives.

---

## ğŸ™ Remerciements

- Enseignant du cours de Machine Learning : Yann Causeur
- CommunautÃ©s open-source (Scikit-Learn, SHAP, Fairlearn)
- Dataset Portmap original (pour la Partie 1)

---

**DerniÃ¨re mise Ã  jour :** 11 DÃ©cembre 2025
